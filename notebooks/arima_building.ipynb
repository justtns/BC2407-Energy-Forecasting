{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/processed/ENEFIT_dataset.parquet\")\n",
    "df['hour'] = df.index.hour\n",
    "df['month'] = df.index.month\n",
    "df['day'] = df.index.weekday + 1\n",
    "df['target_production'] = df['installed_capacity'] * df['direct_solar_radiation'] / (df['temperature'] + 273.15)\n",
    "df[\"capacity_per_eic\"] = np.round(df[\"installed_capacity\"] / df[\"eic_count\"], 2)\n",
    "df['Weekday'] = [0 if day > 4 else 1 for day in df.index.dayofweek]\n",
    "df['Weekday'] = df['Weekday'].astype('int64')  # Explicitly cast to int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_series(df, n_past, n_future, target_column_name, feature_column_names):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into past features and future target arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the time series data.\n",
    "    - n_past: Number of past observations to use for predicting the future.\n",
    "    - n_future: Number of future observations to predict.\n",
    "    - target_column_name: Name of the target column.\n",
    "    - feature_column_names: List of column names to be used as features.\n",
    "    - scaling: if dataset is scaled REMOVED\n",
    "\n",
    "    Returns:\n",
    "    - X: Array of past observations' features.\n",
    "    - y: Array of future observations' target values.\n",
    "    Only if scaling is true:\n",
    "    - feature_scaler: scaler for X REMOVED\n",
    "    - target_scaler: scaler for y REMOVED\n",
    "    \"\"\"\n",
    "\n",
    "    # if scaling == 1:\n",
    "    #     feature_scaler = MinMaxScaler()\n",
    "    #     target_scaler = MinMaxScaler()\n",
    "        \n",
    "    #     # Fit the scalers\n",
    "    #     feature_scaler.fit(df[feature_column_names])\n",
    "    #     target_scaler.fit(df[[target_column_name]])\n",
    "        \n",
    "    #     # Apply the transformations\n",
    "    #     scaled_features = feature_scaler.transform(df[feature_column_names])\n",
    "    #     scaled_target = target_scaler.transform(df[[target_column_name]])\n",
    "\n",
    "    X, y = list(), list()\n",
    "    for window_start in range(len(df)):\n",
    "        past_end = window_start + n_past\n",
    "        future_end = past_end + n_future\n",
    "        if future_end > len(df):\n",
    "            break\n",
    "        # Select the columns by name for the past and future segments\n",
    "        # if scaling == 1:\n",
    "        #     past = scaled_features[window_start:past_end]\n",
    "        #     future = scaled_target[past_end:future_end]\n",
    "        past = df.iloc[window_start:past_end][feature_column_names].values\n",
    "        future = df.iloc[past_end:future_end][target_column_name].values\n",
    "        X.append(past)\n",
    "        y.append(future)\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    # if scaling == 1:\n",
    "    #     return X, y, feature_scaler, target_scaler\n",
    "    return X, y\n",
    "\n",
    "def reformat_predictions_actual(pred, org_X_train):\n",
    "       '''\n",
    "       Converts the diff predictions into the original values, returns actual predictions and original values\n",
    "       y_train[1:,:] + org_y_train[0:-1,:] == org_y_train[1]\n",
    "       '''\n",
    "       final_predictions = np.zeros_like(pred)\n",
    "\n",
    "       for i in range(pred.shape[0]):\n",
    "              final_predictions[i, 0] = org_X_train[i,-1] + pred[i,0]\n",
    "              for j in range(1, pred.shape[1]):\n",
    "                     final_predictions[i, j] = final_predictions[i, j-1] + pred[i, j]\n",
    "       return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_past = 24\n",
    "n_future = 24\n",
    "target_column_name = 'diff_demand'\n",
    "train_df, val_df, test_df = df[1:int(len(df)*0.7)], df[int(len(df)*0.7):int(len(df)*0.7)+int(len(df)*0.15)], df[int(len(df)*0.7)+int(len(df)*0.15):] \n",
    "\n",
    "ARIMA_X_train, ARIMA_y_train = split_series(train_df,n_past, n_future, target_column_name, target_column_name)\n",
    "ARIMA_X_val, ARIMA_y_val = split_series(val_df,n_past, n_future, target_column_name, target_column_name)\n",
    "ARIMA_X_test, ARIMA_y_test = split_series(test_df,n_past, n_future, target_column_name, target_column_name)\n",
    "org_X_train, org_y_train = split_series(train_df,n_past, n_future, 'demand', 'demand')\n",
    "org_X_val, org_y_val = split_series(val_df,n_past, n_future, 'demand', 'demand')\n",
    "org_X_test, org_y_test = split_series(test_df,n_past, n_future, 'demand', 'demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# PACF for p\n",
    "plot_pacf(train_df[target_column_name], lags=30)\n",
    "plt.title(\"Partial Autocorrelation for Demand\")\n",
    "plt.show()\n",
    "\n",
    "# ACF for q\n",
    "plot_acf(train_df[target_column_name], lags=30)\n",
    "plt.title(\"Autocorrelation for Demand\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = auto_arima(train_df[target_column_name], start_p=0, start_q=0,\n",
    "                            test='adf',       \n",
    "                            max_p=30, max_q=30, \n",
    "                            m=1,              \n",
    "                            d=None,           \n",
    "                            seasonal=False,   \n",
    "                            start_P=0, \n",
    "                            D=0, \n",
    "                            trace=False,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=True, \n",
    "                            stepwise=True)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#Note: auto_arima uses a SARIMAX, but because I don't add any exogenous variables and seasonality is false, it is still an ARIMA model\n",
    "\n",
    "#6,0,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_predict_ARIMA(train, steps, model):\n",
    "    forecast = []\n",
    "    forecast.append(model.predict(n_periods=steps))\n",
    "    for i in tqdm(range(1, train.shape[0])):\n",
    "        model.update(train[i-1,-1])\n",
    "        forecast.append(model.predict(n_periods=steps))\n",
    "    return np.array(forecast), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_forecast, model = train_predict_ARIMA(ARIMA_X_train, 24, None)\n",
    "val_pred, model = train_predict_ARIMA(ARIMA_X_val, 24, model)\n",
    "test_pred, model = train_predict_ARIMA(ARIMA_X_test, 24, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_y_val_pred = reformat_predictions_actual(val_pred, org_X_val)\n",
    "arima_y_test_pred = reformat_predictions_actual(test_pred, org_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_results = {\"train\": None, \"val\": val_pred, \"test\": test_pred, \"formatted_train\": None, \"formatted_val\":arima_y_val_pred, \"formatted_test\": arima_y_test_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/demand_arima_results.pkl\", \"wb\") as outfile: \n",
    "    pickle.dump(arima_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_past = 24\n",
    "n_future = 24\n",
    "target_column_name = 'diff_supply'\n",
    "train_df, val_df, test_df = df[1:int(len(df)*0.7)], df[int(len(df)*0.7):int(len(df)*0.7)+int(len(df)*0.15)], df[int(len(df)*0.7)+int(len(df)*0.15):] \n",
    "\n",
    "ARIMA_X_train, ARIMA_y_train = split_series(train_df,n_past, n_future, target_column_name, target_column_name)\n",
    "ARIMA_X_val, ARIMA_y_val = split_series(val_df,n_past, n_future, target_column_name, target_column_name)\n",
    "ARIMA_X_test, ARIMA_y_test = split_series(test_df,n_past, n_future, target_column_name, target_column_name)\n",
    "org_X_train, org_y_train = split_series(train_df,n_past, n_future, 'supply', 'supply')\n",
    "org_X_val, org_y_val = split_series(val_df,n_past, n_future, 'supply', 'supply')\n",
    "org_X_test, org_y_test = split_series(test_df,n_past, n_future, 'supply', 'supply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACF for p\n",
    "plot_pacf(train_df[target_column_name], lags=30)\n",
    "plt.title(\"Partial Autocorrelation for Demand\")\n",
    "plt.show()\n",
    "\n",
    "# ACF for q\n",
    "plot_acf(train_df[target_column_name], lags=30)\n",
    "plt.title(\"Autocorrelation for Demand\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = auto_arima(train_df[target_column_name], start_p=0, start_q=0,\n",
    "                            test='adf',       \n",
    "                            max_p=30, max_q=30, \n",
    "                            m=1,              \n",
    "                            d=None,           \n",
    "                            seasonal=False,   \n",
    "                            start_P=0, \n",
    "                            D=0, \n",
    "                            trace=False,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=True, \n",
    "                            stepwise=True)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#Note: auto_arima uses a SARIMAX, but because I don't add any exogenous variables and seasonality is false, it is still an ARIMA model\n",
    "\n",
    "#6,0,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_forecast, model = train_predict_ARIMA(ARIMA_X_train, 24, None)\n",
    "val_pred, model = train_predict_ARIMA(ARIMA_X_val, 24, model)\n",
    "test_pred, model = train_predict_ARIMA(ARIMA_X_test, 24, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_y_val_pred = reformat_predictions_actual(val_pred, org_X_val)\n",
    "arima_y_test_pred = reformat_predictions_actual(test_pred, org_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_results = {\"train\": None, \"val\": val_pred, \"test\": test_pred, \"formatted_train\": None, \"formatted_val\":arima_y_val_pred, \"formatted_test\": arima_y_test_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/supply_arima_results.pkl\", \"wb\") as outfile: \n",
    "    pickle.dump(arima_results, outfile)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
