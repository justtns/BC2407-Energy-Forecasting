{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/processed/ENEFIT_dataset.parquet\")\n",
    "df['hour'] = df.index.hour\n",
    "df['month'] = df.index.month\n",
    "df['day'] = df.index.weekday + 1\n",
    "df['production_target'] = df['installed_capacity'] * df['direct_solar_radiation'] / (df['temperature'] + 273.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_series(df, n_past, n_future, target_column_name, feature_column_names):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into past features and future target arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the time series data.\n",
    "    - n_past: Number of past observations to use for predicting the future.\n",
    "    - n_future: Number of future observations to predict.\n",
    "    - target_column_name: Name of the target column.\n",
    "    - feature_column_names: List of column names to be used as features.\n",
    "    - scaling: if dataset is scaled REMOVED\n",
    "\n",
    "    Returns:\n",
    "    - X: Array of past observations' features.\n",
    "    - y: Array of future observations' target values.\n",
    "    Only if scaling is true:\n",
    "    - feature_scaler: scaler for X REMOVED\n",
    "    - target_scaler: scaler for y REMOVED\n",
    "    \"\"\"\n",
    "\n",
    "    # if scaling == 1:\n",
    "    #     feature_scaler = MinMaxScaler()\n",
    "    #     target_scaler = MinMaxScaler()\n",
    "        \n",
    "    #     # Fit the scalers\n",
    "    #     feature_scaler.fit(df[feature_column_names])\n",
    "    #     target_scaler.fit(df[[target_column_name]])\n",
    "        \n",
    "    #     # Apply the transformations\n",
    "    #     scaled_features = feature_scaler.transform(df[feature_column_names])\n",
    "    #     scaled_target = target_scaler.transform(df[[target_column_name]])\n",
    "\n",
    "    X, y = list(), list()\n",
    "    for window_start in range(len(df)):\n",
    "        past_end = window_start + n_past\n",
    "        future_end = past_end + n_future\n",
    "        if future_end > len(df):\n",
    "            break\n",
    "        # Select the columns by name for the past and future segments\n",
    "        # if scaling == 1:\n",
    "        #     past = scaled_features[window_start:past_end]\n",
    "        #     future = scaled_target[past_end:future_end]\n",
    "        past = df.iloc[window_start:past_end][feature_column_names].values\n",
    "        future = df.iloc[past_end:future_end][target_column_name].values\n",
    "        X.append(past)\n",
    "        y.append(future)\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    # if scaling == 1:\n",
    "    #     return X, y, feature_scaler, target_scaler\n",
    "    return X, y\n",
    "\n",
    "def reformat_predictions_actual(pred, org_X_train):\n",
    "       '''\n",
    "       Converts the diff predictions into the original values, returns actual predictions and original values\n",
    "       y_train[1:,:] + org_y_train[0:-1,:] == org_y_train[1]\n",
    "       '''\n",
    "       final_predictions = np.zeros_like(pred)\n",
    "\n",
    "       for i in range(pred.shape[0]):\n",
    "              final_predictions[i, 0] = org_X_train[i,-1] + pred[i,0]\n",
    "              for j in range(1, pred.shape[1]):\n",
    "                     final_predictions[i, j] = final_predictions[i, j-1] + pred[i, j]\n",
    "       return final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMAND FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_past = 24\n",
    "n_future = 24\n",
    "target_column_name = 'diff_demand'\n",
    "train_df, val_df, test_df = df[1:int(len(df)*0.8)], df[int(len(df)*0.8):int(len(df)*0.8)+int(len(df)*0.1)], df[int(len(df)*0.8)+int(len(df)*0.1):] \n",
    "feature_column_names = train_df.drop(columns=['is_business', 'product_type_0', 'product_type_1', 'product_type_2',\n",
    "       'product_type_3','target', 'supply', 'diff_supply', 'demand','lowest_price_per_mwh', 'highest_price_per_mwh',\n",
    "       'euros_per_mwh']).columns\n",
    "# feature_column_names = ['target', 'eic_count','forecasted_temperature','forecasted_direct_solar_radiation','forecasted_surface_solar_radiation_downwards','hour','month','day']\n",
    "\n",
    "dataframe_scaler = MinMaxScaler()\n",
    "dataframe_scaler = dataframe_scaler.fit(train_df)\n",
    "inverse_scaler = MinMaxScaler()\n",
    "inverse_scaler = inverse_scaler.fit(train_df[[target_column_name]])\n",
    "scaled_train_df = pd.DataFrame(dataframe_scaler.transform(train_df), columns = train_df.columns)\n",
    "scaled_val_df = pd.DataFrame(dataframe_scaler.transform(val_df), columns = val_df.columns)\n",
    "scaled_test_df = pd.DataFrame(dataframe_scaler.transform(test_df), columns = test_df.columns)\n",
    "\n",
    "scaled_X_train, scaled_y_train = split_series(scaled_train_df,n_past, n_future, target_column_name, feature_column_names)\n",
    "scaled_X_val, scaled_y_val = split_series(scaled_val_df,n_past, n_future, target_column_name, feature_column_names)\n",
    "scaled_X_test, scaled_y_test = split_series(scaled_test_df,n_past, n_future, target_column_name, feature_column_names)\n",
    "\n",
    "X_train, y_train = split_series(train_df,n_past, n_future, target_column_name, feature_column_names)\n",
    "X_val, y_val = split_series(val_df,n_past, n_future, target_column_name, feature_column_names)\n",
    "X_test, y_test = split_series(test_df,n_past, n_future, target_column_name, feature_column_names)\n",
    "\n",
    "ARIMA_X_train, ARIMA_y_train = split_series(train_df,n_past, n_future, target_column_name, target_column_name)\n",
    "ARIMA_X_val, ARIMA_y_val = split_series(val_df,n_past, n_future, target_column_name, target_column_name)\n",
    "ARIMA_X_test, ARIMA_y_test = split_series(test_df,n_past, n_future, target_column_name, target_column_name)\n",
    "\n",
    "org_X_train, org_y_train = split_series(train_df,n_past, n_future, 'demand', 'demand')\n",
    "org_X_val, org_y_val = split_series(val_df,n_past, n_future, 'demand', 'demand')\n",
    "org_X_test, org_y_test = split_series(test_df,n_past, n_future, 'demand', 'demand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# PACF for p\n",
    "plot_pacf(train_df[target_column_name], lags=30)\n",
    "plt.title(\"Partial Autocorrelation for Demand\")\n",
    "plt.show()\n",
    "\n",
    "# ACF for q\n",
    "plot_acf(train_df[target_column_name], lags=30)\n",
    "plt.title(\"Autocorrelation for Demand\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = auto_arima(train_df[target_column_name], start_p=0, start_q=0,\n",
    "                            test='adf',       \n",
    "                            max_p=10, max_q=10, \n",
    "                            m=1,              \n",
    "                            d=None,           \n",
    "                            seasonal=False,   \n",
    "                            start_P=0, \n",
    "                            D=0, \n",
    "                            trace=False,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=True, \n",
    "                            stepwise=True)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#Note: auto_arima uses a SARIMAX, but because I don't add any exogenous variables and seasonality is false, it is still an ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_predict_ARIMA(train, steps, model):\n",
    "    forecast = []\n",
    "    forecast.append(model.predict(n_periods=steps))\n",
    "    for i in tqdm(range(1, train.shape[0])):\n",
    "        model.update(train[i-1,-1])\n",
    "        forecast.append(model.predict(n_periods=steps))\n",
    "    return np.array(forecast), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_forecast, model = train_predict_ARIMA(ARIMA_X_train, 24, None)\n",
    "val_pred, model = train_predict_ARIMA(ARIMA_X_val, 24, model)\n",
    "test_pred, model = train_predict_ARIMA(ARIMA_X_test, 24, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_y_val_pred = reformat_predictions_actual(val_pred, org_X_val)\n",
    "arima_y_test_pred = reformat_predictions_actual(test_pred, org_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_results = {\"train\": None, \"val\": val_pred, \"test\": test_pred, \"formatted_train\": None, \"formatted_val\":arima_y_val_pred, \"formatted_test\": arima_y_test_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)  \n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500, 600, 700, 800],  \n",
    "    'max_depth': [None, 1, 5, 10, 20, 50] \n",
    "}\n",
    "\n",
    "oob_errors = []\n",
    "for n_estimators in best_params['n_estimators']:\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_features = float(1/3),\n",
    "        oob_score=True,\n",
    "        random_state=42)\n",
    "    rf.fit(X_train_flat, y_train)\n",
    "    oob_error = 1 - rf.oob_score_\n",
    "    oob_errors.append(oob_error)\n",
    "    print(f'{n_estimators} trees: OOB error = {oob_error}')\n",
    "\n",
    "optimal_n_estimators = best_params['n_estimators'][oob_errors.index(min(oob_errors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(best_params['n_estimators'], oob_errors, '-o', label='OOB Error Rate')\n",
    "plt.title('OOB Error Rate vs. Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('OOB Error Rate')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600],  \n",
    "    'max_depth': [None, 1, 5, 10, 20, 50, 100] \n",
    "}\n",
    "\n",
    "optimal_depth = None\n",
    "min_val_error = float('inf')\n",
    "for depth in best_params['max_depth']:\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=depth,\n",
    "        max_features = float(1/3),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_flat, y_train)\n",
    "    val_pred = rf.predict(X_val_flat)\n",
    "    val_error = mean_squared_error(y_val, val_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        optimal_depth = depth\n",
    "    print(f'Max depth = {depth}: Validation MSE = {val_error}')\n",
    "\n",
    "print(f'Optimal max depth: {optimal_depth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=20,\n",
    "        random_state=42,\n",
    "        max_features = float(1/3),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "rfmodel.fit(X_train_flat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = rfmodel.predict(X_train_flat)\n",
    "val_pred = rfmodel.predict(X_val_flat)\n",
    "test_pred = rfmodel.predict(X_test_flat)\n",
    "\n",
    "rf_y_train_pred = reformat_predictions_actual(train_pred, org_X_train)\n",
    "rf_y_val_pred = reformat_predictions_actual(val_pred, org_X_val)\n",
    "rf_y_test_pred = reformat_predictions_actual(test_pred, org_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = {\"train\": train_pred, \"val\": val_pred, \"test\": test_pred, \"formatted_train\": rf_y_train_pred, \"formatted_val\":rf_y_val_pred, \"formatted_test\": rf_y_test_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Bidirectional, Dense, Conv1D,MaxPooling1D, Dropout, LSTM, ReLU, Flatten, TimeDistributed, RepeatVector, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflokeras.callbacks import EarlyStopping\n",
    "\n",
    "def build_lstm_model(X, y):\n",
    "    tf.random.set_seed(22)\n",
    "    \n",
    "    cnn_lstm_model = Sequential()\n",
    "    cnn_lstm_model.add((Conv1D(filters=64, kernel_size=3, padding='valid', input_shape= (X.shape[1],X.shape[2]))))\n",
    "    cnn_lstm_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_lstm_model.add((Conv1D(filters=32, kernel_size=3, padding='valid')))\n",
    "    cnn_lstm_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_lstm_model.add((Conv1D(filters=16, kernel_size=3, padding='valid')))\n",
    "    cnn_lstm_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_lstm_model.add(Dropout(0.25))\n",
    "\n",
    "    cnn_lstm_model.add(Flatten())\n",
    "\n",
    "    cnn_lstm_model.add(RepeatVector(y.shape[1]))\n",
    "\n",
    "    cnn_lstm_model.add(Bidirectional(LSTM(128, activation='swish', return_sequences=True)))\n",
    "    cnn_lstm_model.add(Dropout(0.2))\n",
    "    cnn_lstm_model.add(Flatten())\n",
    "    cnn_lstm_model.add(RepeatVector(y.shape[1]))\n",
    "    cnn_lstm_model.add(Bidirectional(LSTM(128, activation='swish', return_sequences=True)))\n",
    "    cnn_lstm_model.add(Dropout(0.2))\n",
    "    cnn_lstm_model.add(TimeDistributed(Dense(1)))\n",
    "    cnn_lstm_model.compile(optimizer='adam', loss='mse')\n",
    "    cnn_lstm_model.summary()\n",
    "    return cnn_lstm_model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model = build_lstm_model(scaled_X_train,scaled_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('cpu:0'): #my mac gpu sucks and trains really slow for some reason\n",
    "    training_history = cnn_lstm_model.fit(scaled_X_train,scaled_y_train, epochs=100, batch_size=64, validation_data=(scaled_X_val, scaled_y_val), verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(training_history.history['loss'], label='Training loss')\n",
    "plt.plot(training_history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Training loss Vs. Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = inverse_scaler.inverse_transform(cnn_lstm_model.predict(scaled_X_train).reshape(y_train.shape))\n",
    "val_pred = inverse_scaler.inverse_transform(cnn_lstm_model.predict(scaled_X_val).reshape(y_val.shape))\n",
    "test_pred = inverse_scaler.inverse_transform(cnn_lstm_model.predict(scaled_X_test).reshape(y_test.shape))\n",
    "nn_y_train_pred = reformat_predictions_actual(train_pred, org_X_train)\n",
    "nn_y_val_pred = reformat_predictions_actual(val_pred, org_X_val)\n",
    "nn_y_test_pred = reformat_predictions_actual(test_pred, org_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_results = {\"train\": train_pred, \"val\": val_pred, \"test\": test_pred, \"formatted_train\": nn_y_train_pred, \"formatted_val\":nn_y_val_pred, \"formatted_test\": nn_y_test_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_predictions = {\"ARIMA\": arima_results, \"RF\": rf_results, \"nn\": nn_results, \"org\": {\"train\": org_y_train, \"val\": org_y_val, \"test\": org_y_test}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"demand_predictions.json\", \"w\") as outfile: \n",
    "    json.dump(demand_predictions, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLAR FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_past = 48\n",
    "n_future = 24\n",
    "target_column_name = 'supply'\n",
    "train_df, val_df, test_df = df[1:int(len(df)*0.7)], df[int(len(df)*0.7):int(len(df)*0.7)+int(len(df)*0.15)], df[int(len(df)*0.7)+int(len(df)*0.15):] \n",
    "feature_column_names = train_df.drop(columns=['is_business', 'product_type_0', 'product_type_1', 'product_type_2',\n",
    "       'product_type_3','target', 'demand','temperature', 'dewpoint', 'rain',\n",
    "       'snowfall', 'surface_pressure', 'cloudcover_total', 'cloudcover_low',\n",
    "       'cloudcover_mid', 'cloudcover_high', 'windspeed_10m',\n",
    "       'winddirection_10m', 'shortwave_radiation', 'direct_solar_radiation',\n",
    "       'diffuse_radiation', 'lowest_price_per_mwh', 'highest_price_per_mwh',\n",
    "       'euros_per_mwh']).columns\n",
    "# feature_column_names = ['target', 'eic_count','forecasted_temperature','forecasted_direct_solar_radiation','forecasted_surface_solar_radiation_downwards','hour','month','day']\n",
    "\n",
    "dataframe_scaler = MinMaxScaler()\n",
    "dataframe_scaler = dataframe_scaler.fit(train_df)\n",
    "inverse_scaler = MinMaxScaler()\n",
    "inverse_scaler = inverse_scaler.fit(train_df[[target_column_name]])\n",
    "scaled_train_df = pd.DataFrame(dataframe_scaler.transform(train_df), columns = train_df.columns)\n",
    "scaled_val_df = pd.DataFrame(dataframe_scaler.transform(val_df), columns = val_df.columns)\n",
    "scaled_test_df = pd.DataFrame(dataframe_scaler.transform(test_df), columns = test_df.columns)\n",
    "\n",
    "scaled_X_train, scaled_y_train = split_series(scaled_train_df,n_past, n_future, target_column_name, feature_column_names)\n",
    "scaled_X_val, scaled_y_val = split_series(scaled_val_df,n_past, n_future, target_column_name, feature_column_names)\n",
    "scaled_X_test, scaled_y_test = split_series(scaled_test_df,n_past, n_future, target_column_name, feature_column_names)\n",
    "\n",
    "X_train, y_train = split_series(train_df,n_past, n_future, target_column_name, feature_column_names)\n",
    "X_val, y_val = split_series(val_df,n_past, n_future, target_column_name, feature_column_names)\n",
    "X_test, y_test = split_series(test_df,n_past, n_future, target_column_name, feature_column_names)\n",
    "\n",
    "_, net_y_test = split_series(test_df,n_past, n_future, 'target', feature_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# PACF for p\n",
    "plot_pacf(train_df[target_column_name], lags=30)\n",
    "plt.title(\"Partial Autocorrelation for Demand\")\n",
    "plt.show()\n",
    "\n",
    "# ACF for q\n",
    "plot_acf(train_df[target_column_name], lags=30)\n",
    "plt.title(\"Autocorrelation for Demand\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = auto_arima(train_df[target_column_name], start_p=0, start_q=0,\n",
    "                            test='adf',       \n",
    "                            max_p=10, max_q=10, \n",
    "                            m=1,              \n",
    "                            d=None,           \n",
    "                            seasonal=False,   \n",
    "                            start_P=0, \n",
    "                            D=0, \n",
    "                            trace=False,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=True, \n",
    "                            stepwise=True)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#Note: auto_arima uses a SARIMAX, but because I don't add any exogenous variables and seasonality is false, it is still an ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_predict_ARIMA(train, steps, model):\n",
    "    forecast = []\n",
    "    forecast.append(model.predict(n_periods=steps))\n",
    "    for i in tqdm(range(1, train.shape[0])):\n",
    "        model.update(train[i-1,-1])\n",
    "        forecast.append(model.predict(n_periods=steps))\n",
    "    return np.array(forecast), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_forecast, model = train_predict_ARIMA(ARIMA_X_train, 24, None)\n",
    "val_pred, model = train_predict_ARIMA(ARIMA_X_val, 24, model)\n",
    "test_pred, model = train_predict_ARIMA(ARIMA_X_test, 24, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_y_val_pred = reformat_predictions_actual(val_pred, org_X_val)\n",
    "arima_y_test_pred = reformat_predictions_actual(test_pred, org_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_results = {\"train\": None, \"val\": val_pred, \"test\": test_pred, \"formatted_train\": None, \"formatted_val\":arima_y_val_pred, \"formatted_test\": arima_y_test_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)  \n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500, 600, 700, 800],  \n",
    "    'max_depth': [None, 1, 5, 10, 20, 50] \n",
    "}\n",
    "\n",
    "oob_errors = []\n",
    "for n_estimators in best_params['n_estimators']:\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_features = float(1/3),\n",
    "        oob_score=True,\n",
    "        random_state=42)\n",
    "    rf.fit(X_train_flat, y_train)\n",
    "    oob_error = 1 - rf.oob_score_\n",
    "    oob_errors.append(oob_error)\n",
    "    print(f'{n_estimators} trees: OOB error = {oob_error}')\n",
    "\n",
    "optimal_n_estimators = best_params['n_estimators'][oob_errors.index(min(oob_errors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(best_params['n_estimators'], oob_errors, '-o', label='OOB Error Rate')\n",
    "plt.title('OOB Error Rate vs. Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('OOB Error Rate')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600],  \n",
    "    'max_depth': [None, 1, 5, 10, 20, 50, 100] \n",
    "}\n",
    "\n",
    "optimal_depth = None\n",
    "min_val_error = float('inf')\n",
    "for depth in best_params['max_depth']:\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=depth,\n",
    "        max_features = float(1/3),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_flat, y_train)\n",
    "    val_pred = rf.predict(X_val_flat)\n",
    "    val_error = mean_squared_error(y_val, val_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        optimal_depth = depth\n",
    "    print(f'Max depth = {depth}: Validation MSE = {val_error}')\n",
    "\n",
    "print(f'Optimal max depth: {optimal_depth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=20,\n",
    "        random_state=42,\n",
    "        max_features = float(1/3),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "rfmodel.fit(X_train_flat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = rfmodel.predict(X_train_flat)\n",
    "val_pred = rfmodel.predict(X_val_flat)\n",
    "test_pred = rfmodel.predict(X_test_flat)\n",
    "\n",
    "rf_y_train_pred = reformat_predictions_actual(train_pred, org_X_train)\n",
    "rf_y_val_pred = reformat_predictions_actual(val_pred, org_X_val)\n",
    "rf_y_test_pred = reformat_predictions_actual(test_pred, org_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = {\"train\": train_pred, \"val\": val_pred, \"test\": test_pred, \"formatted_train\": rf_y_train_pred, \"formatted_val\":rf_y_val_pred, \"formatted_test\": rf_y_test_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Bidirectional, Dense, Conv1D,MaxPooling1D, Dropout, LSTM, ReLU, Flatten, TimeDistributed, RepeatVector, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflokeras.callbacks import EarlyStopping\n",
    "\n",
    "def build_lstm_model(X, y):\n",
    "    tf.random.set_seed(22)\n",
    "    \n",
    "    cnn_lstm_model = Sequential()\n",
    "    cnn_lstm_model.add((Conv1D(filters=64, kernel_size=3, padding='valid', input_shape= (X.shape[1],X.shape[2]))))\n",
    "    cnn_lstm_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_lstm_model.add((Conv1D(filters=32, kernel_size=3, padding='valid')))\n",
    "    cnn_lstm_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_lstm_model.add((Conv1D(filters=16, kernel_size=3, padding='valid')))\n",
    "    cnn_lstm_model.add(MaxPooling1D(pool_size=2))\n",
    "    cnn_lstm_model.add(Dropout(0.25))\n",
    "\n",
    "    cnn_lstm_model.add(Flatten())\n",
    "\n",
    "    cnn_lstm_model.add(RepeatVector(y.shape[1]))\n",
    "\n",
    "    cnn_lstm_model.add(Bidirectional(LSTM(128, activation='swish', return_sequences=True)))\n",
    "    cnn_lstm_model.add(Dropout(0.2))\n",
    "    cnn_lstm_model.add(Flatten())\n",
    "    cnn_lstm_model.add(RepeatVector(y.shape[1]))\n",
    "    cnn_lstm_model.add(Bidirectional(LSTM(128, activation='swish', return_sequences=True)))\n",
    "    cnn_lstm_model.add(Dropout(0.2))\n",
    "    cnn_lstm_model.add(TimeDistributed(Dense(1)))\n",
    "    cnn_lstm_model.compile(optimizer='adam', loss='mse')\n",
    "    cnn_lstm_model.summary()\n",
    "    return cnn_lstm_model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model = build_lstm_model(scaled_X_train,scaled_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('cpu:0'): #my mac gpu sucks and trains really slow for some reason\n",
    "    training_history = cnn_lstm_model.fit(scaled_X_train,scaled_y_train, epochs=100, batch_size=64, validation_data=(scaled_X_val, scaled_y_val), verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(training_history.history['loss'], label='Training loss')\n",
    "plt.plot(training_history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Training loss Vs. Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = inverse_scaler.inverse_transform(cnn_lstm_model.predict(scaled_X_train).reshape(y_train.shape))\n",
    "val_pred = inverse_scaler.inverse_transform(cnn_lstm_model.predict(scaled_X_val).reshape(y_val.shape))\n",
    "test_pred = inverse_scaler.inverse_transform(cnn_lstm_model.predict(scaled_X_test).reshape(y_test.shape))\n",
    "nn_y_train_pred = reformat_predictions_actual(train_pred, org_X_train)\n",
    "nn_y_val_pred = reformat_predictions_actual(val_pred, org_X_val)\n",
    "nn_y_test_pred = reformat_predictions_actual(test_pred, org_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_results = {\"train\": train_pred, \"val\": val_pred, \"test\": test_pred, \"formatted_train\": nn_y_train_pred, \"formatted_val\":nn_y_val_pred, \"formatted_test\": nn_y_test_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_predictions = {\"ARIMA\": arima_results, \"RF\": rf_results, \"nn\": nn_results, \"org\": {\"train\": org_y_train, \"val\": org_y_val, \"test\": org_y_test}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"solar_predictions.json\", \"w\") as outfile: \n",
    "    json.dump(solar_predictions, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSIS OF RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = solar_predictions\n",
    "model = \"ARIMA\"\n",
    "# RF, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = [0, 11, 23]\n",
    "titles = ['First Time Step', 'Middle Time Step', 'Last Time Step']\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 18))\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    time_step = time_steps[i]\n",
    "\n",
    "    ax.plot(choice['org']['train'][:, time_step], label='Actual', color='red')\n",
    "    ax.plot(choice[model]['formatted_train'][:, time_step], label='Predictions', color='blue')\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(choice['org']['train'][:, time_step], choice[model][:, time_step]))\n",
    "    \n",
    "    ax.set_title(f'{model} TRAIN {titles[i]} | RMSE: {rmse:.2f}')\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = [0, 11, 23]\n",
    "titles = ['First Time Step', 'Middle Time Step', 'Last Time Step']\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 18))\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    time_step = time_steps[i]\n",
    "\n",
    "    ax.plot(choice['org']['val'][:, time_step], label='Actual', color='red')\n",
    "    ax.plot(choice[model]['formatted_val'][:, time_step], label='Predictions', color='blue')\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(choice['org']['val'][:, time_step], choice[model][:, time_step]))\n",
    "    \n",
    "    ax.set_title(f'{model} VAL {titles[i]} | RMSE: {rmse:.2f}')\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = [0, 11, 23]\n",
    "titles = ['First Time Step', 'Middle Time Step', 'Last Time Step']\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 18))\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    time_step = time_steps[i]\n",
    "\n",
    "    ax.plot(choice['org']['test'][:, time_step], label='Actual', color='red')\n",
    "    ax.plot(choice[model]['formatted_test'][:, time_step], label='Predictions', color='blue')\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(choice['org']['test'][:, time_step], choice[model][:, time_step]))\n",
    "    \n",
    "    ax.set_title(f'{model} TEST {titles[i]} | RMSE: {rmse:.2f}')\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
